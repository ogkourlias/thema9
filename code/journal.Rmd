---
title: "Journal: What is the most accurate model in predicting the stroke risk of a person given 10 attributes."
author: "Orfeas Gkourlias"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    toc: true
    fig_caption: yes
    df_print: paged
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(ggplot2)
library(lemon)
library(knitr)
library(RWeka)
library(DMwR)
library(Hmisc)
require(caTools)
codebook <- read_delim(file = "codebook.txt",
                       delim = ";",
                       show_col_types=FALSE)
```
\newpage

# Introduction
This document aims to explore, analyse and explain the data set being used
in answering the following research question: "What is the most accurate model
in predicting the stroke risk of a person given 10 attributes?". As the research
question implies, the data set consists of 10 attributes. This project
has the goal of analyzing those attributes, so that the best
machine learning algorithm can be trained. Some attributes affect each other,
while others may not. Analysis of these correlations can help in finding the
rankings of the attributes. The final answer to the research question will be found using Weka,
a data platform which utilizes machine learning.

To get a feel for what the scope and attributes of the data set consists of, it
will be loaded and the first 10 results will be displayed.

```{r}
main <- read.csv("../data/stroke-data.csv")
head(main)
nrow(main)
```

There are 12 attributes, 10 of which will be used in the analysis: Gender, age
hypertension, heart_disease, ever_married, work_type, residence_type, avg_glucose_level,
BMI and smoking_status. The last column indicates whether the person has 
already experienced a prior stroke. This can be used to the train the machine learning
model which will be utilized to answer the research question.

There are 5110 entries in this data set. This is also why the row numbers will
not be replaced with the id's, because there is no order in the id numbers. They
exceed the number 5110.

The attributes and their units can be seen in the code book on the next page.
\newpage   

# Codebook.
```{r}
knitr::kable(codebook)
```
\newpage
# 1. Exploratory data analysis.
The exploratory data analysis consists of multiple sections. In the first section,
the raw data will be observed and interpreted. Any cleaning up or filtering of the
data will also be performed in this first section. Following the first section will
be the exploration of correlations in the data. This part will explain how the
attributes may affect each other, and whether any trends can be observed.

## 1.1. Initial data and attributes.
In this section, the attributes will be examined individually. What these attributes
could mean for the research question will be discussed. 
Any pre-processing or cleanup required will also be performed in this section.

### ID.
This column is neither noteworthy for analysis or data structure. This column
will therefore be dropped, because the data frame used already has row numbers
and this makes the ID redundant.

```{r}
main <- main[2:12]
```

### Age.
The age of the patient. At first sight, it might look redundant for this data to
be stored as a float, since most of the data consists of a rounded age number.
Some of the entries contain very young patients. The younger a patient is, the 
more important the specificity of the age is, since the age difference is
still significant at that point. It is for that reason that any patient under the
age of 2 will contain a float number, with two decimal numbers. A couple of those
instances will be shown in vector format below:

```{r}
head(c(main[main$age < 2, 2]))
```
The likelihood of a person experiencing a stroke increases with age. This will
therefore be an important attribute in the analysis.

### Gender.
Gender, containing three separate values: Male, Female and other. There could be
a correlation in the gender and stroke risk of a person. An example of gender
indirectly affecting the stroke risk of a patient would be the age difference
in genders. Women tend to live longer lives than men, therefore the stroke risk
may increase for women, because they generally become older. This could be one
of many hypothesis.

```{r}
main$gender <- factor(main$gender)
```

\newpage

### Hypertension.
This indicates with a 0 or 1 whether the patient is affected by hypertension.
The first attribute which is relevant to the heart status of a patient. These
types of attributes will always be important, because any heart condition tends
to come with an increased risk of experiencing a stroke. Since this is indicated
with a binary number, the patient either has hypertension, which is indicated with a 1, or not, which 
is indicated with a 0. Correlations will probably be found between hypertension and
the other attributes. In this case, it will be easier to work with "Yes" or "No"
values. Not only for the sake of consistency, but because Weka otherwise recognizes
the 0's and 1's as numeric values, not boolean values This will therefore be changed
with the following code:

```{r}
main$hypertension <- factor(main$hypertension, labels = c("No", "Yes"))
```

The datatypes were also changed to that of a factor. This has been done so that
later functions will properly work on the data.

### Heart Disease.
Similar to the previous attribute, this is also an important element when
trying to predict stroke risk. The previous observation also applies to this 
attribute.

```{r}
main$heart_disease <- factor(main$heart_disease, labels = c("No", "Yes"))
```

### Ever married.
This displays whether the patient has ever been married in their lifetime. This
will most likely not bet detrimental in predicting the stroke risks of patients.
But this is part of the data set, so it will therefore be compared with the other
attributes, to see where it ranks with it's prediction.

```{r}
main$ever_married <- factor(main$ever_married, labels = c("No", "Yes"))
```

### Work Type.
A similar attribute to the prior one. Will most likely not be a good predictor
for stroke risk. But it may rank higher than the marriage attribute. Some sectors
could theoretically expose a person to environments where strokes are more likely.

```{r}
main$work_type <- factor(main$work_type)
```

\newpage

### Residence Type.
Considering that some types of residency might be healthier than others, this
attribute may be slightly important in determining the stroke risk of a person.

```{r}
main$Residence_type <- factor(main$Residence_type)
```

### Average Glucose Level.
This may be more important than the prior three attributes, especially when these
levels are unusually low or high. The literature concerning glucose levels and
their connection to strokes is still being debated. Some papers conclude that
it is not detrimental when observed in non-diabetic people.

### BMI.
The body mass index is an indicator for how a person's weight/height ratio. Age 
and gender also being taken into consideration for the calculation. Both extreme
ends of this attribute could be important to the stroke risk of a person. Higher
BMI levels are also associated with developing heart disease. Glucose levels may also
be affected. Several other attributes are most likely going to have a correlation
to this attribute.

It is worth noting that this is the only attribute with missing values. This
can be shown with a summary. Before doing that however, it seems that the BMI
column consists of characters, not numbers. This must first be changed.
```{r}
typeof(main$bmi)
main$bmi <- as.numeric(main$bmi)
summary(main$bmi)
```
There appears to be 201 missing values. Imputation will be chosen to remedy this.
Because of the insignificant amount, and the attribute being a numerical one, 
making it easier to fill in missing data properly. This will be done with the 
usage of the "Hmisc" package. Median will be chosen for the new values because
the median is not affected by extreme values, unlike mean. Extreme values are plenty
under the BMI attribute in this dataset.

```{r}
main$bmi <- as.vector(impute(main$bmi, median))
main$bmi <- round(main$bmi, 1)
```

### Smoking Status.
Whether a patient is smoking will most likely affect some of the other attribute
s in this data set. Whether these significant correlations will need to be tested.
The smoking status is unknown for some of the patients.

```{r}
main$smoking_status <- factor(main$smoking_status)
```

### Stroke.
The column indicating whether the patient has ever had a stroke. This is the
classifier which will be predicted for in the final model, with the highest
possible accuracy.

```{r}
main$stroke <- factor(main$stroke, labels = c("No", "Yes"))
```

\newpage
## 1.2. Correlations.
Following the examination of the initial values and attributes, they may
now be compared so that trends and correlations can be observed. Starting
with the first attribute, which will most likely be important in determining
stroke risk, age. By plotting the summaries of patients who had a stroke, and those
who did not, maybe a link can be seen between the two.

```{r age_by_stroke, fig.cap="Boxplots of age distribution grouped by stroke status."}
no_stroke <- main[main$stroke == "No",]
stroke <- main[main$stroke == "Yes",]
ggplot(main, aes(x=factor(stroke), y=age))+geom_boxplot()+
  ggtitle("Ages grouped by stroke history")+
  theme(plot.title = element_text(hjust = 0.5), plot.caption = element_text(hjust=0),
        axis.title.x = element_text(margin = unit(c(5, 0, 0, 0), "mm")))+
  xlab("Stroke Status")+
  ylab("Age")
```

\newpage

This plot shows that the group of people who have had strokes are, on average,
older than the group who has not experienced a stroke. This may also be observed
with a summary
```{r}
summary(no_stroke$age)
summary(stroke$age)
```
It is worth noting that the no stroke group does contain plenty of patients of higher
age.

Now, a t-test may be performed to determine whether age is a significant difference
between the two groups. A one sample t-test is most appropriate here, considering that
every patient's risk is independent from another

```{r}
t.test(no_stroke$age, stroke$age)[3]
```
Without considering the other attributes, it seems that with that p-value, the
two age means are significantly different.

These tests may be performed for all of the other attributes too. Showing them all
individually would be redundant, since the correlations between having a stroke and
the other attributes relating to the heart already have literature confirming them.

\newpage

The attributes, which may seem less influential at first would be worth checking
manually. The marriage status being the first one to consider. This column is not
a boolean number, even though it is comparable to the other 1 or 0 columns. For
consistency, yes will become 1 and no will become 0.

```{r marriage_and_stroke, fig.cap="Barplots of normalized marriage status grouped by stroke status"}
no_stroke <- main[main$stroke == "No",]
stroke <- main[main$stroke == "Yes",]

no_s_table <- log(table(no_stroke$ever_married))
s_table <- log(table(stroke$ever_married))

vec <- c(no_s_table[1], no_s_table[2], s_table[1], s_table[2])

barplot(vec, space = c(0,0,1,0), col = c("brown4", "cadetblue"), 
        names.arg = c("                       No Stroke","",
                      "                       Stroke",""), ylab = "Log(Occurance)",
        main = "Normalized marriage numbers comparison")
legend("topright", c("Not Married", "Married"), fill = c("brown4", "cadetblue"))
```


The data has been normalized, so that it may be properly displayed. The group of
people with no stroke quite lower than the other group. This plot shows that for
both groups, the amount of people who have ever been married is bigger than the
not married group. It makes sense for there to be proportionally more people who have married
than less when looking at the stroke bars. This is because people who have had
a stroke are on average older than the other group. It is also safe to assume that
the older a person is, the more likely that they have ever been married in their life.

\newpage

Now, it is possible to make a similar plot, but with a more relevant attribute.
Heart disease would be a good choice.
```{r heart_and_stroke, fig.cap="Barplots of normalized heart disease status grouped by stroke status"}
no_s_table <- log(table(no_stroke$heart_disease))
s_table <- log(table(stroke$heart_disease))

vec <- c(no_s_table[1], no_s_table[2], s_table[1], s_table[2])

barplot(vec, space = c(0,0,1,0), col = c("brown4", "cadetblue"), 
        names.arg = c("                       No Stroke","",
                      "                       Stroke",""), ylab = "Log(Occurance)",
        main = "Normalized heart status comparison")
legend("topright", c("No Heart disease", "Heart Disease"),
       fill = c("brown4", "cadetblue"))
```

\newpage

The assumption would be that a heart disease has a correlation with having a stroke.
The bar plot above does not show anything out of the ordinary. It might be
difficult to find any correlation by just looking at these bar plots. Calculating
the differing odds between two groups could help.

```{r}
ratio1 <- as.vector(table(no_stroke$heart_disease)[2])/nrow(no_stroke)
ratio2 <- as.vector(table(stroke$heart_disease)[2])/nrow(stroke)
ratio1
ratio2
```
In this case, it does appear that the ratio is different. A calculation can be
made to determine how much more likely patients with heart conditions may get
a stroke.

```{r}
ratio2/ratio1
```
Judging by that simple calculation, a patient with a heart condition has 4 times
the likelihood of getting a stroke than someone without a heart condition.

\newpage

A bar plot showing the correlation between BMI and other attributes could
also show correlations.

```{r bmi_by_stroke, fig.cap="Boxplots of BMI distribution grouped by stroke status."}
ggplot(main, aes(x=factor(stroke), y=bmi))+geom_boxplot()+ggtitle("BMI by stroke status")+
  xlab("Stroke")+
  theme(plot.title = element_text(hjust = 0.5), plot.caption = element_text(hjust=0),
        axis.title.x = element_text(margin = unit(c(5, 0, 0, 0), "mm")))
```

\newpage

## 1.3. Overview.
In finishing the exploratory data analysis section of the journal, more insight was
gained into the implications of the data and the attribute properties. Correlations
were also found, especially when observing the attributes related to cardiology.

The data is a good candidate for analysis using machine learning algorithms, now
that the data has been polished and trimmed.

```{r}
path_out = "../data/"
write.csv(main, paste(path_out, "main.csv", sep = ""), row.names = FALSE)
```

\newpage

# 2. Machine Learning & Weka.
In this section, the various ways of training a machine learning algorithm will
be tried and tested. The data frame will first be prepared for Weka usage. Second
follows an examination of what results will be important considering the data set.
So that the most important metrics can be accounted for. This will all be done
to make guarantee that the maximum potential of the algorithms is being observed
in the third and last section, in which the program Weka will be used to train machine
learning models for predicting the stroke attribute.

## 2.1. Weka.
The data set will soon be put through weka algorithm testing. The goal here is to train
an algorithm which will have the most accuracy when it comes to stroke prediction.
RWeka is a package which allows for some Weka procedures to be performed in R.
Specifically the writing of .arff files from data frames. Whenever a procedure
was performed in the Weka program itself, it will be specified in this document.

The main file is currently a .csv file. Weka works better with .arff files, so
it will therefore be copied to an .arff format for future use.
```{r}
RWeka::write.arff(main, file = paste(path_out, "main.arff", sep = ""))
```

## 2.2. Important Metrics.
The accuracy of a model will be most important, followed by the sensitivity The
following section will explain why detecting true negatives and the specificity,
are so important in this case.

### Specificity
When looking at the significance of false positives and negatives, the effect of
both must be considered relative to their data set. In this case, false positive
and negatives both have differing degrees of severity. When a false positive gets
detected, a patient will falsely be classified as being at risk of having a stroke.
Using a machine learning model as the only diagnosis method should never
be the only course of action. It is therefore safe to assume that followup
medical examinations of the patient will be performed, allowing for the false p
positive to be corrected. In this worst case, when the false positive does not
get corrected, a patient may have to adjust their lifestyle and take medication.
There are no negatives to the first solution, since the majority of lifestyle 
adjustments will overlap with the general consensus on what it means to live
a healthy life. But the potential of medication would be problematic, because exposing
someone to unnecessary medication could endanger a patient's health.

Now, to compare that to the consequences of a false negative. In the case of a false
negative, a person who is at risk of getting a stroke, will be classified a someone
who is not actually at risk. This means that the person will have to go through life
with an ever increasing risk, since the risk also gets higher the older a person gets.
The implication being that the person will eventually suffer an unexpected stroke, which
will most likely lead to life debilitating consequences, or even death.

It is for that reason that the false negatives are weighted significantly stricter than
false positives, because the outcomes for a patient are so detrimental for the latter.
This is the reasoning as to why the sensitivity will be considered more in
training a model over specificity.


### Data imbalance and ZeroR
Something else which is of high importance is the data imbalance. To properly test
the model, there needs to be more entries where the stroke class indicates "Yes".
Otherwise, the ZeroR algorithm will always net the highest accuracy, which makes sense
with the current imbalance. 95% of the data entries have not yet had a stroke.
ZeroR will therefore offer the highest accuracy of 95% with this balance.

It is for that reason that an oversampling technique was chosen to help 
combat this problem. Synthetic Minority Oversampling Technique (SMOTE) will
be used to this end. SMOTE can be performed using the DMwR package.

Another important part of properly training a model is separation between train
and test data sets. In this case, there is only one data set present. This data set
can be divided into two separate files, so that one may be used for training
purposes and the other for testing purposes. An algorithm will be applied to
the first, then the resulting model will be tested on the latter. 

SMOTE may not be applied on the training data set. The data will first be divided
and then have the sampling technique applied to the training data. 1/4th of the
data will be used for testing, and the remaining for training.

```{r}
set.seed(101) 
sample = sample.split(main$stroke, SplitRatio = .75)
train = subset(main, sample == TRUE)
test  = subset(main, sample == FALSE)
train <- SMOTE(stroke ~., train, perc.over = 1800, perc.under = 200)
table(main$stroke)
table(train$stroke)
```
The tables show the difference pre and post SMOTE application. Both the subsets
will be written to separate .arff files.

```{r}
RWeka::write.arff(train, file = paste(path_out, "train.arff", sep = ""))
RWeka::write.arff(test, file = paste(path_out, "test.arff", sep = ""))
```

Performing ZeroR on this new .arff file gives an accuracy of 65%. This is
still an imbalance, but not as severe as before. Models may now be trained using
different classifiers.

## 2.3. Weka Model Exploration.

Following the separation and SMOTE application, all the following algorithm
exploration will be done in the following order: The algorithm will be trained
using the SMOTE training set. Then the test set will be the supplied test set
in weka. The resulting accuracy will be the result of that execution.

## Naive Bayes
Applying a default Naive Bayes on the data sets provides an accuracy of 77.603%.
With 991 correctly classified instances and 286 incorrectly classified instances.
The attribute that is being predicted for is a binary one. The application of the
"Naive Bayes Multinominal Text" algorithm provide a higher accuracy of 95.14%.
This is already a quite impressive result being the first of the linear classifiers.

## K-Nearest Neighbors
This algorithm is named as "IBk" in weka, meaning instanced based learner.
Using the default settings on the test, an accuracy of 78.856% is reached. While
better than the default Naive Bayes test, it is not better than the multinominal
text variation. Changing the nearest neighbor search algorithm from the default
linear NN search does not net any better results.

Now some decision tree algorithms will be explored.

## Decision Stump
Default execution of this algorithm nets an accuracy of 59.122%. Any alteration to
the settings is unlikely to show a significant increase in accuracy when the
default is already this low.

## J48
This algorithm results in a 85.905% accuracy. Playing with the settings either
gives a similar or lower accuracy. Looking at other decision tree algorithms
will most likely provide higher accuracy.

## Random tree
The random tree algorithm gives a 77.291% accuracy. Changing the break ties setting
from false to true, allows for slightly better accuracy. The result of this is 
79.404%. Changing other settings does not provide a better accuracy.

## Random forest
A random forest algorithm results in 81.989% accuracy. The only t hing affecting
the accuracy to a significant degree is the number of iterations, which gives 
a percentage of increase in accuracy.