---
title: "What is the most accurate model in predicting the stroke risk of a person given 10 attributes."
author: "Orfeas Gkourlias - 420172"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: no
    toc_depth: 2
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(ggplot2)
library(tidyr)
library(lemon)
library(knitr)
codebook <- read_delim(file = "codebook.txt",
                       delim = ";",
                       show_col_types=FALSE)
main <- read.csv("../data/main.csv", check.names=FALSE)
no_stroke <- main[main$stroke == "No",]
stroke <- main[main$stroke == "Yes",]
no_s_table <- log(table(no_stroke$heart_disease))
s_table <- log(table(stroke$heart_disease))

vec <- c(no_s_table[1], no_s_table[2], s_table[1], s_table[2])

ratio1 <- as.vector(table(no_stroke$heart_disease)[2])/nrow(no_stroke)
ratio2 <- as.vector(table(stroke$heart_disease)[2])/nrow(stroke)
```
\newpage

\section{Preface}
The project and paper were written for an assignment given by Hanzehogeschool
Groningen. With the educational goal of providing a learning experience regarding
machine learning and some Java. We were given the opportunity to choose any
data set to our liking for the project, adhering to the requirements set, of course.

This made it so that the entire experience was more interesting, because of the 
agency in choosing something that interested me. Sadly, the first data set I had
chosen did not meet the requirements, resulting in me lagging behind for a little while.
Luckily, I was able to catch up with another interesting subject, concerning stroke patients. 
This subject intrigues me tremendously, because of people in my environment who 
have had the misfortune of suffering a stroke. Considering the ever growing importance
and emphasizes on preventative care, and the significance of strokes on people, 
I thought this would be a perfect candidate for the machine learning project.
I would like to thank my teachers for the guidance they offered in execution of the 
project.

\newpage

\section{Abstract}
Every 40 seconds someone in the united states suffers from a stroke. With around
1 out of 5 people passing away as a result. The outlook for stroke patients are
also varying in severity for the survivors. With extensive aftercare almost being
a guarantee in all cases, strokes are also the leading cause of serious, long-term
disabilities and impairments. With only 1 out of 10 patients making a full recovery.

The severity and prevalence of strokes cannot be understated. While modern aftercare
usually provides favorable prognosis, the complete prevention of not only strokes, but
many cardiovascular conditions has long been advocated for. The importance of preventative
healthcare keeps being re-emphasized as people grow older. There are many preventative
measures available and still being developed.

Considering the above, the ability to predict whether someone may suffer from a stroke
would be an invaluable tool. This project has aimed to do just that. By taking
patient data, examining that and then training a machine learning model to make
predictions based off it. Using the powerful machine-learning and data analysis
program called Weka, a prediction model was developed and trained to be 92 percent
accurate in its classification. With emphasis on avoiding false negatives, the
model will be identify high risk patients with utmost care.

If this model could be made even more accurate, then it might serve as a primary
detector of high risk stroke patients. Although a wide array of different 
methodologies and algorithms were tried, there might still be facets of the 
project which could have been done differently. Further examination of the model
would maybe be able to push it past even the 95 percent accuracy mark.

To compliment the model, a java program was also developed. This program takes the 
model, new instances of data with similar structure, and makes a prediction
on all of the instances. This way many entries from big data-sets can be classified
at once.

\newpage

\section{Abbreviations}
SMOTE : Synthetic Minority Oversampling Technique
BMI : BOdy mass index
\newpage

\section{Table of Contents}
```{=latex}
\setcounter{tocdepth}{4}
\tableofcontents
```
\newpage

\section{Introduction}
Stroke is one of the leading causes of death and disability worldwide, ranking
at second for amount of deaths and third for death and disabilities combined
[World Stroke Organization (WSO): Global Stroke Fact Sheet 2022](https://pubmed.ncbi.nlm.nih.gov/34986727/).
The best solution to this worldwide problem will be preventative care. As high
as 80% of all the stroke cases are preventable ones. [Preventing Stroke Deaths](https://www.cdc.gov/vitalsigns/pdf/2017-09-vitalsigns.pdf)
The correlation between stroke risk and attributes such as BMI and blood pressure
are already well observed. These metrics are useful in predicting a person's likelihood 
of suffering from a stroke. While this may be done on an individual basis, a computer 
could in theory also make these predictions, given the required metrics. This is 
where machine learning and statistical inference could be paramount in preventative care.
The total amount of stroke patients may be reduced significantly,by applying a 
machine learning model and predicting for multiple people whether they are at risk
of suffering from a stroke.

## Objective
The objective of this project is to develop and train a model, which can accurately
predict whether a person is likely to suffer from a stroke. This can be achieved
using machine learning algorithms, which are already present in the program named
[Weka](https://www.cs.waikato.ac.nz/ml/weka/). This final model may then be used
in a Java program, which predicts for every given person whether they are
likely to suffer from a stroke.

## Theory
As mentioned before, preventing strokes will be the most efficient way to combat
them. This can be done in many ways, but detecting that on a large scale is already
quite an undertaking. Doctors can already provide sufficient diagnosis of attributes
such as elevated blood pressures and increased BMIs. But to do this for every
single individual would require too many people. Patients are often already unaware
of elevated blood pressures, hence why it is called a silent killer. Application of
a machine learning model allows for the diagnosis process to be applied on a greater
scale. Taking thousands of patient and predicting their individual risks is faster
and more efficient.

Machine learning can be a complicated process, with many different ways of
approaching the training of a model. First training data needs to be collected.
In this case, data was taken from [kaggle](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset?resource=download).
This data contains 12 attributes, with one being the classifier: stroke history.
5110 instances are present in the main dataset.

```{r, echo=F}
knitr::kable(codebook)
```
Some of the attributes can directly be identified as correlated to an increased
risk of having a stroke. Other attributes are less obvious contenders,
such as the marriage status. The last attribute is the one for which will be
predicted. In this case, it is a binary classification, with stroke either indicating
yes or no. This was an important consideration when choosing an algorithm, because
not every algorithm is as effective as another when it comes to classification
problems. The accuracy, meaning how many of the instances a model predicts correctly
is of high importance. Something else which was closely examined was the false
negative rate. This rate must be as low as possible without compromising much
of the accuracy, because false negatives are more destructive to a person than
a false positive in this case. The ROC curve was therefore also an important facet
of the end result. 

While the data itself is of good quality, there was one concern to be had. The
classifier was quite uneven in distribution. With 95% of the instances being 
classified as no, with the remaining 5% being a no. While maybe realistic, it would
skew the final model too much, resulting in a lower accuracy. Bringing the balance
closer to 70/30 would be better for training purposes. To achieve this, the
synthetic minority oversampling technique (SMOTE) was used. As the technique implies,
it increases the number of total cases in a balanced way.

The final data is then prepared for model training, which as mentioned before
will be aimed at increasing the accuracy while reducing the false negative rate.
\newpage

\section{Materials and methods}
The entirety of this project, including both the machine learning and java part
, was developed in a [github repository.](https://github.com/ogkourlias/thema9)
The ml directory contains the machine learning section, with the java directory
containing the entire java program. This project was also performed by one student.

## Material
The project consists of multiple programs and tools. The versions of the software
and tools are important, especially in the case of the java program. A table
will be shown with all the software which was used.

```{=latex}
\begin{table}[thb]
\begin{tabular}{|l|l|l|}
\hline
\textbf{Software} & \textbf{Version} & \textbf{Function}                \\ \hline
R                 & 4.2.1            & Statistical analysis language    \\ \hline
Weka              & 3.8.6            & Machine learning platform        \\ \hline
Java              & 17.0.5           & Application programming language \\ \hline
Gradle            & 7.4              & Application builder              \\ \hline
\end{tabular}
\end{table}
```


\section{Results}
Following the analysis of the data set and the attributes within, some important
correlations can be observed. Correlations relevant to the research question
mostly consist of heart and health attributes. The first plot regarding heart
disease differences in non-stroke and stroke patients will be shown.

```{r, echo=F}
barplot(vec, space = c(0,0,1,0), col = c("brown4", "cadetblue"), 
        names.arg = c("                       No Stroke","",
                      "                       Stroke",""), ylab = "Log(Occurance)",
        main = "Heart disease status by stroke history")
legend("topright", c("No Heart disease", "Heart Disease"),
       fill = c("brown4", "cadetblue"))
```

This plot of normalized data shows that there are proportionally more heart disease
patients in the stroke group, than there are in the no stroke group. This ratio
can be translated to a percentage. The percentage of
patients who have not had a stroke is `r round(ratio1*100, 2)`%. This percen
tage is higher for the group of patients who have experienced a stroke. For that
group the percentage is `r round(ratio2*100, 2)`%. This means that it is 4 times as
likely for someone who has had a stroke, to also have a heart condition.
The literature on this subject seems to have come to a common consensus: There is
a correlation between having a heart disease and a higher risk of a stroke, This 
paper published by the American Heart Association being one of the bigger
published works on the topic: [Heart Disease and Stroke Statistics—2020 Update: A Report From the American Heart Association](https://www.ahajournals.org/doi/full/10.1161/CIR.0000000000000757?rfr_dat=cr_pub++0pubmed&url_ver=Z39.88-2003&rfr_id=ori%3Arid%3Acrossref.org)
(Keep in mind, the risk of developing heart disease AFTER  a stroke is also quite big.)

The same can be seen in the case of hypertension, because that is also
a heart condition.

```{r, echo=F}
no_s_table <- log(table(no_stroke$hypertension))
s_table <- log(table(stroke$hypertension))

vec <- c(no_s_table[1], no_s_table[2], s_table[1], s_table[2])

barplot(vec, space = c(0,0,1,0), col = c("brown4", "cadetblue"), 
        names.arg = c("                       No Stroke","",
                      "                       Stroke",""), ylab = "Log(Occurance)",
        main = "Heart disease status by stroke history")
legend("topright", c("No Hypertension", "Hypertension"),
       fill = c("brown4", "cadetblue"))
```
This plot is very similar to the prior one. That makes sense, because quite some
heart conditions are accompanied by hypertension. Hypertension on its own
is already classified as a heart condition too.

\newpage
Age has been observed to also be an important attribute. The older a person beco
mes, the higher the likelihood of a stroke. Especially when combined with other
attributes, such as hypertension or heart conditions. According to this paper, 
the odds of having a stroke double for every ten years a person lives. 
[Aging and ischemic stroke](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6535078/)

```{r, echo=F}
ggplot(main, aes(x=factor(stroke), y=age))+geom_boxplot()+ggtitle("Ages by stroke status")+
  xlab("Stroke")
```
This shows that the older a person is, the more likely they are to have experienced a stroke.
This would of course make sense, considering that the older someone gets, the 
higher the chance one might encounter health issues.

Another health related observation that may be relevant is that of the BMI means
for stroke and no stroke patients. This can be visualized through a similar box plot
as the prior one.

```{r, echo=F}
ggplot(main, aes(x=factor(stroke), y=bmi))+geom_boxplot()+ggtitle("BMI by stroke status")+
  xlab("Stroke")
```
The patients who have had a stroke only have a slightly higher BMI. The no stroke
group also has more extreme values on both ends.

## Imbalance
Something important which can be observed in the data is the imbalance between
stroke instances. A barplot will be used to demonstrate this imbalance

```{r, echo=F}
ggplot(data = main) +   geom_bar(mapping = aes(x = stroke))+ ggtitle("Stroke No/Yes Comparison")
ratio <- round(nrow(main[main$stroke == 0,]) / nrow(main), 3)
```

As can be seen, there is a great imbalance in the amount of patients who
have had a stroke those who have not. To be exact: `r ratio*100`% of the data,
consists of rows where stroke is equal to 0. The remaining 5% has experienced
a stroke. This will be very important when selecting for different machine learn
ing algorithms, because the weighting of occurrences will most likely have to be
changed in this case.

\newpage

\section{Discussion and Conclusion}
Now that the most important results have been shown, some points of contention
could be considered. The aim of this project is to answer the original research
question using machine learning. This may be done with varying algorithms and 
considering different attributes. Looking at the results, some of the attribute
s may seem irrelevant at first, and some would argue these could be removed before
applying any machine learning algorithms. But in this case, none of the attributes
will be discarded. The program being used, Weka, has multiple functions which allow
for the automation of this process. By allowing Weka to select the most influential
attributes systematically, biases may be avoided.

In addition to the data selection, the imbalance is also important to consider.
One way to tackle this problem is by generating samples in the minority class.
That class being the 1 instances under the stroke attribute. Another option was
to use SMOTE (Synthetic Minority Oversampling Technique).  This method was chosen
because it is effectively a more accurate version of oversampling. SMOTE will 
generate synthetic data points, so that the absence of balance will be remedied.
Whether this should be remedied might also be debatable. Some may argue that the
raw data ratio would be an accurate representation. But considering that the 
difference is this extreme, SMOTE was chosen regardless.

\newpage

\section{References}

\newpage

\section{Appendices}